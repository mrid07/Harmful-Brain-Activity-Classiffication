{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>3636015289</td>\n",
       "      <td>57</td>\n",
       "      <td>272.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>87</td>\n",
       "      <td>724.0</td>\n",
       "      <td>4060339532</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>3636015289</td>\n",
       "      <td>58</td>\n",
       "      <td>280.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>88</td>\n",
       "      <td>732.0</td>\n",
       "      <td>2591107590</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>3636015289</td>\n",
       "      <td>59</td>\n",
       "      <td>286.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>89</td>\n",
       "      <td>738.0</td>\n",
       "      <td>1384712649</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>3636015289</td>\n",
       "      <td>60</td>\n",
       "      <td>298.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>90</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1528528240</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>3636015289</td>\n",
       "      <td>61</td>\n",
       "      <td>300.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>91</td>\n",
       "      <td>752.0</td>\n",
       "      <td>4044602144</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>839643180</td>\n",
       "      <td>120</td>\n",
       "      <td>568.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>217</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>3295148391</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>839643180</td>\n",
       "      <td>121</td>\n",
       "      <td>572.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>218</td>\n",
       "      <td>1594.0</td>\n",
       "      <td>3536587712</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>839643180</td>\n",
       "      <td>122</td>\n",
       "      <td>576.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>219</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>1537073604</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>4233815620</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>220</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>3914505153</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>4233815620</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12849827</td>\n",
       "      <td>221</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>2309348431</td>\n",
       "      <td>34998</td>\n",
       "      <td>GPD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n",
       "515  3636015289          57                     272.0        12849827   \n",
       "516  3636015289          58                     280.0        12849827   \n",
       "517  3636015289          59                     286.0        12849827   \n",
       "518  3636015289          60                     298.0        12849827   \n",
       "519  3636015289          61                     300.0        12849827   \n",
       "..          ...         ...                       ...             ...   \n",
       "645   839643180         120                     568.0        12849827   \n",
       "646   839643180         121                     572.0        12849827   \n",
       "647   839643180         122                     576.0        12849827   \n",
       "648  4233815620           0                       0.0        12849827   \n",
       "649  4233815620           1                       2.0        12849827   \n",
       "\n",
       "     spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n",
       "515                  87                             724.0  4060339532   \n",
       "516                  88                             732.0  2591107590   \n",
       "517                  89                             738.0  1384712649   \n",
       "518                  90                             750.0  1528528240   \n",
       "519                  91                             752.0  4044602144   \n",
       "..                  ...                               ...         ...   \n",
       "645                 217                            1590.0  3295148391   \n",
       "646                 218                            1594.0  3536587712   \n",
       "647                 219                            1598.0  1537073604   \n",
       "648                 220                            1988.0  3914505153   \n",
       "649                 221                            1990.0  2309348431   \n",
       "\n",
       "     patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  lrda_vote  \\\n",
       "515       34998              GPD             3         0        12          0   \n",
       "516       34998              GPD             3         0        12          0   \n",
       "517       34998              GPD             3         0        12          0   \n",
       "518       34998              GPD             3         0        12          0   \n",
       "519       34998              GPD             3         0        12          0   \n",
       "..          ...              ...           ...       ...       ...        ...   \n",
       "645       34998              GPD             0         0         4          0   \n",
       "646       34998              GPD             0         0         4          0   \n",
       "647       34998              GPD             0         0         4          0   \n",
       "648       34998              GPD             3         0         8          0   \n",
       "649       34998              GPD             3         0         8          0   \n",
       "\n",
       "     grda_vote  other_vote  \n",
       "515          0           0  \n",
       "516          0           0  \n",
       "517          0           0  \n",
       "518          0           0  \n",
       "519          0           0  \n",
       "..         ...         ...  \n",
       "645          0           1  \n",
       "646          0           1  \n",
       "647          0           1  \n",
       "648          0           2  \n",
       "649          0           2  \n",
       "\n",
       "[135 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('./new_train.csv')\n",
    "# \n",
    "df[515:650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Creating subid parquet files***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category GPD\n",
      "Processing row 500 for category GPD\n",
      "Processing row 600 for category GPD\n",
      "Processing row 700 for category GPD\n",
      "Processing row 800 for category GPD\n",
      "Processing row 1000 for category GPD\n",
      "Processing row 2000 for category GPD\n",
      "Processing row 2300 for category GPD\n",
      "Processing row 2900 for category GPD\n",
      "Processing row 3600 for category GPD\n",
      "Processing row 3700 for category GPD\n",
      "Processing category GRDA\n",
      "Processing row 300 for category GRDA\n",
      "Processing row 1300 for category GRDA\n",
      "Processing row 2200 for category GRDA\n",
      "Processing row 2700 for category GRDA\n",
      "Processing row 2800 for category GRDA\n",
      "Processing row 3400 for category GRDA\n",
      "Processing row 4000 for category GRDA\n",
      "Processing row 4500 for category GRDA\n",
      "Processing row 4600 for category GRDA\n",
      "Processing row 4700 for category GRDA\n",
      "Processing category LPD\n",
      "Processing row 1100 for category LPD\n",
      "Processing row 1700 for category LPD\n",
      "Processing row 1800 for category LPD\n",
      "Processing row 3300 for category LPD\n",
      "Processing row 3800 for category LPD\n",
      "Processing row 3900 for category LPD\n",
      "Processing row 4100 for category LPD\n",
      "Processing row 4400 for category LPD\n",
      "Processing row 7100 for category LPD\n",
      "Processing category LRDA\n",
      "Processing row 400 for category LRDA\n",
      "Processing row 1200 for category LRDA\n",
      "Processing row 1400 for category LRDA\n",
      "Processing row 1500 for category LRDA\n",
      "Processing row 1600 for category LRDA\n",
      "Processing row 2600 for category LRDA\n",
      "Processing row 4200 for category LRDA\n",
      "Processing row 5000 for category LRDA\n",
      "Processing row 5100 for category LRDA\n",
      "Processing row 5400 for category LRDA\n",
      "Processing row 6100 for category LRDA\n",
      "Processing row 6200 for category LRDA\n",
      "Processing category Other\n",
      "Processing row 100 for category Other\n",
      "Processing row 200 for category Other\n",
      "Processing row 2100 for category Other\n",
      "Processing row 2400 for category Other\n",
      "Processing row 3500 for category Other\n",
      "Processing row 4800 for category Other\n",
      "Processing row 5300 for category Other\n",
      "Processing row 5600 for category Other\n",
      "Processing row 6300 for category Other\n",
      "Processing row 6400 for category Other\n",
      "Processing category Seizure\n",
      "Processing row 0 for category Seizure\n",
      "Processing row 900 for category Seizure\n",
      "Processing row 1900 for category Seizure\n",
      "Processing row 2500 for category Seizure\n",
      "Processing row 3000 for category Seizure\n",
      "Processing row 3100 for category Seizure\n",
      "Processing row 3200 for category Seizure\n",
      "Processing row 4300 for category Seizure\n",
      "Processing row 5200 for category Seizure\n",
      "Processing row 5500 for category Seizure\n",
      "Processing row 5800 for category Seizure\n",
      "Processed and saved 1000 files per expert_consensus category.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "metadata = pd.read_csv('./train.csv')\n",
    "\n",
    "# Define the sampling rate (200 Hz = 200 samples per second)\n",
    "sampling_rate = 200\n",
    "\n",
    "# Defining the duration of the window to extract (50 seconds)\n",
    "window_duration = 50 * sampling_rate  \n",
    "\n",
    "output_dir = 'testingFiles/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "files_per_category = 1000\n",
    "\n",
    "grouped_metadata = metadata.groupby('expert_consensus')\n",
    "\n",
    "for expert_consensus, group in grouped_metadata:\n",
    "    print(f\"Processing category {expert_consensus}\")\n",
    "    \n",
    "    category_files = group.head(files_per_category)\n",
    "    \n",
    "    for i, row in category_files.iterrows():\n",
    "        if i % 100 == 0:  \n",
    "            print(f\"Processing row {i} for category {expert_consensus}\")\n",
    "\n",
    "        eeg_id = row['eeg_id']\n",
    "        eeg_sub_id = row['eeg_sub_id']\n",
    "        offset_seconds = row['eeg_label_offset_seconds']\n",
    "        eeg_data = pd.read_parquet(f'./train_eegs/{eeg_id}.parquet')\n",
    "\n",
    "        offset_samples = int(offset_seconds * sampling_rate)\n",
    "\n",
    "        # Extract the relevant portion of the EEG data (50 seconds of data)\n",
    "        eeg_subsample = eeg_data.iloc[offset_samples:offset_samples + window_duration]\n",
    "\n",
    "        output_file = f'{output_dir}/{eeg_id}_{eeg_sub_id}.parquet'\n",
    "\n",
    "        # Saving subsample as a new .parquet file\n",
    "        eeg_subsample.to_parquet(output_file, index=False)\n",
    "\n",
    "print(f\"Processed and saved 1000 files per expert_consensus category.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***File to generate train.csv file for the selected data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train.csv created with 6000 entries based on available files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "metadata = pd.read_csv('./train.csv')\n",
    "\n",
    "file_dir = './testingFiles/'\n",
    "\n",
    "filtered_rows = []\n",
    "\n",
    "# Looping through each row in the metadata and check if the corresponding file exists\n",
    "for i, row in metadata.iterrows():\n",
    "    eeg_id = row['eeg_id']\n",
    "    eeg_sub_id = row['eeg_sub_id']\n",
    "\n",
    "    file_path = f'{file_dir}/{eeg_id}_{eeg_sub_id}.parquet'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # If the file exists, add the row to the filtered list\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "filtered_metadata = pd.DataFrame(filtered_rows)\n",
    "\n",
    "filtered_metadata.to_csv('new_train.csv', index=False)\n",
    "\n",
    "print(f\"New train.csv created with {len(filtered_metadata)} entries based on available files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 199\n",
      "Processing row 398\n",
      "Processing row 597\n",
      "Processing row 796\n",
      "Processing row 995\n",
      "Processing row 1194\n",
      "Processing row 1393\n",
      "Processing row 1592\n",
      "Processing row 1791\n",
      "Processing row 1990\n",
      "Processing row 2189\n",
      "Processing row 2388\n",
      "Processing row 2587\n",
      "Processing row 2786\n",
      "Processing row 2985\n",
      "Processing row 3184\n",
      "Processing row 3383\n",
      "Processing row 3582\n",
      "Processing row 3781\n",
      "Processing row 3980\n",
      "Processing row 4179\n",
      "Processing row 4378\n",
      "Processing row 4577\n",
      "Processing row 4776\n",
      "Processing row 4975\n",
      "Processing row 5174\n",
      "Processing row 5373\n",
      "Processing row 5572\n",
      "Processing row 5771\n",
      "Processing row 5970\n",
      "Processing complete. All data loaded into X_master and y_master.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_master = pd.DataFrame()  # To store features (EEG subsamples)\n",
    "y_master = pd.DataFrame()  # To store labels (votes)\n",
    "chunk_size = 1000  \n",
    "num_files_to_save = 6000  \n",
    "\n",
    "metadata = pd.read_csv('./new_train.csv')\n",
    "\n",
    "# Loop through the metadata to extract features and labels\n",
    "for i, row in metadata.iterrows():\n",
    "    if i >= num_files_to_save:\n",
    "        break  \n",
    "    \n",
    "    if i % 500 == 0: \n",
    "        print(\"Processing row\", i)\n",
    "    \n",
    "    eeg_id = row['eeg_id']\n",
    "    eeg_sub_id = row['eeg_sub_id']\n",
    "\n",
    "    # Loading corresponding EEG data from parquet\n",
    "    eeg_data = pd.read_parquet(f'./testingFiles/{eeg_id}_{eeg_sub_id}.parquet')\n",
    "\n",
    "    # Flatten the EEG data and append to X\n",
    "    X_chunk = pd.DataFrame(eeg_data.values.flatten()).transpose()  # Flatten to 1D and ensure it's a row\n",
    "\n",
    "    # Append corresponding label to y\n",
    "    y_chunk = pd.DataFrame([row['expert_consensus']], columns=['expert_consensus'])\n",
    "\n",
    "    # Concatenate the current chunk to the master DataFrame\n",
    "    X_master = pd.concat([X_master, X_chunk], ignore_index=True)\n",
    "    y_master = pd.concat([y_master, y_chunk], ignore_index=True)\n",
    "\n",
    "    # Clear the current chunk after processing (optional, as we don't need to store chunks)\n",
    "    X_chunk = None\n",
    "    y_chunk = None\n",
    "\n",
    "print(\"Processing complete. All data loaded into X_master and y_master.\")\n",
    "\n",
    "\n",
    "X=X_master\n",
    "y=y_master\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded from existing pickle files.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "file_X = 'X_data.pkl'\n",
    "file_y = 'y_data.pkl'\n",
    "\n",
    "# Check if the files already exist\n",
    "if not os.path.exists(file_X) or not os.path.exists(file_y):\n",
    "    # Save X and y to pickle files if they don't exist\n",
    "    with open(file_X, 'wb') as f:\n",
    "        pickle.dump(X_master, f)\n",
    "    with open(file_y, 'wb') as f:\n",
    "        pickle.dump(y_master, f)\n",
    "    print(f\"Data has been saved to {file_X} and {file_y}\")\n",
    "else:\n",
    "    # If the files exist, load X and y from the pickle files\n",
    "    with open(file_X, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    with open(file_y, 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    print(\"Data has been loaded from existing pickle files.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mridu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded from existing pickle file.\n",
      "test Accuracy: 0.5933\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GPD       0.77      0.70      0.73       214\n",
      "        GRDA       0.59      0.68      0.63       181\n",
      "         LPD       0.46      0.67      0.55       188\n",
      "        LRDA       0.79      0.63      0.70       223\n",
      "       Other       0.53      0.31      0.39       201\n",
      "     Seizure       0.48      0.57      0.52       193\n",
      "\n",
      "    accuracy                           0.59      1200\n",
      "   macro avg       0.60      0.59      0.59      1200\n",
      "weighted avg       0.61      0.59      0.59      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # This will transform string labels to numerical values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_file = 'model.pkl'\n",
    "\n",
    "# Check if the model file already exists\n",
    "if not os.path.exists(model_file):\n",
    "    \n",
    "    # Training the Random Forest Classifier\n",
    "    forest = RandomForestClassifier(n_estimators=500,max_depth=10,min_samples_leaf=2,min_samples_split=13,max_features='sqrt', random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "\n",
    "    # Saving  model to a pickle file if it doesn't exist\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(forest, f)\n",
    "    print(f\"Model has been saved to {model_file}\")\n",
    "else:\n",
    "    # If the model file exists, load the model from the pickle file\n",
    "    with open(model_file, 'rb') as f:\n",
    "        forest = pickle.load(f)\n",
    "    print(\"Model has been loaded from existing pickle file.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict on testing set\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1    819\n",
      "2    812\n",
      "5    807\n",
      "4    799\n",
      "0    786\n",
      "3    777\n",
      "Name: count, dtype: int64\n",
      "0\n",
      "3    223\n",
      "0    214\n",
      "4    201\n",
      "5    193\n",
      "2    188\n",
      "1    181\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yy=pd.DataFrame(y_train)\n",
    "\n",
    "value_counts = yy.value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(value_counts)\n",
    "\n",
    "\n",
    "yy=pd.DataFrame(y_test)\n",
    "\n",
    "value_counts = yy.value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(value_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
